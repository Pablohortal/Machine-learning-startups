{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer análisis de los datos\n",
    "\n",
    "## Importación de librerías y datos\n",
    "\n",
    "En estas primeras líneas, estoy llevando a cabo dos pasos esenciales en cualquier proyecto de análisis de datos:\n",
    "\n",
    "1. **Importación de librerías:** Estoy importando las librerías que utilizaré en mi análisis. Cada una de estas tiene un propósito específico:\n",
    "    - `numpy`: Me ofrece soporte para trabajar con arrays y matrices de gran tamaño, además de una gran colección de funciones matemáticas para operar con estos elementos.\n",
    "    - `pandas`: Me proporciona estructuras de datos y herramientas de análisis de datos flexibles y fáciles de usar. Es particularmente útil para manipular y analizar datos numéricos y series temporales.\n",
    "    - `scipy.stats`: Este es un submódulo de la librería SciPy que proporciona un gran número de distribuciones de probabilidad y una creciente biblioteca de funciones estadísticas.\n",
    "    - `seaborn`: Es una librería de visualización de datos en Python basada en matplotlib. Me ofrece una interfaz de alto nivel para dibujar gráficos estadísticos atractivos e informativos.\n",
    "    - `matplotlib.pyplot`: Me ofrece una forma de graficar datos y crear figuras de manera similar a la que ofrece MATLAB.\n",
    "    - `os`: Me proporciona una forma portátil de utilizar la funcionalidad dependiente del sistema operativo, como leer o escribir en el sistema de archivos.\n",
    "\n",
    "2. **Carga de datos:** Estoy cargando mi conjunto de datos 'World_Wide_Unicorn_Startups.csv' en un DataFrame de pandas llamado unicornios. Un DataFrame es una estructura de datos bidimensional de tamaño variable y potencialmente heterogénea. Es una de las estructuras de datos más comúnmente utilizadas en pandas para el análisis y la manipulación de datos. La ruta del archivo la paso como argumento a la función `pd.read_csv()`, que lee un archivo csv y lo convierte en DataFrame.\n",
    "\n",
    "# Vista preliminar de los datos\n",
    "\n",
    "Después de cargar el conjunto de datos, es crucial obtener una visión preliminar de cómo se presentan los datos. Para hacer esto, estoy utilizando el método `.head(20)` del DataFrame unicornios, que devuelve las primeras 20 filas del DataFrame.\n",
    "\n",
    "Aquí tienes un resumen de lo que representa cada columna:\n",
    "\n",
    "- **Company:** El nombre de la empresa.\n",
    "- **Valuation:** La valoración de la empresa en miles de millones de dólares.\n",
    "- **Date:** La fecha en que la empresa alcanzó su estatus de unicornio (valoración de más de 1 billón de dólares).\n",
    "- **Country:** El país donde está ubicada la empresa.\n",
    "- **City:** La ciudad en la que se encuentra la empresa.\n",
    "- **Industry:** La industria a la que pertenece la empresa.\n",
    "- **Investors:** Los inversores de la empresa.\n",
    "- **Year:** El año en que la empresa alcanzó su estatus de unicornio.\n",
    "- **Month:** El mes en que la empresa alcanzó su estatus de unicornio.\n",
    "- **Day:** El día del mes en que la empresa alcanzó su estatus de unicornio.\n",
    "\n",
    "Este es un paso significativo en la exploración y análisis de datos, ya que me permite entender la estructura de mis datos. En particular, puedo ver qué tipo de información contiene cada columna y empezar a pensar en cómo podría utilizarla en mi análisis.\n",
    "\n",
    "# Información sobre los datos\n",
    "\n",
    "El método `.info()` se usa para obtener un resumen conciso del DataFrame. Esto incluye la cantidad de entradas no nulas, el tipo de datos de cada columna y una indicación de cuánta memoria está utilizando el DataFrame.\n",
    "\n",
    "En este caso, el DataFrame unicornios tiene 936 entradas, que van desde el índice 0 al 935. Cada una de las columnas tiene la siguiente información:\n",
    "\n",
    "- **Company:** 936 entradas no nulas de tipo object (cadenas de texto).\n",
    "- **Valuation:** 936 entradas no nulas de tipo float64 (números con decimales).\n",
    "- **Date:** 936 entradas no nulas de tipo object (cadenas de texto).\n",
    "- **Country:** 936 entradas no nulas de tipo object (cadenas de texto).\n",
    "- **City:** 921 entradas no nulas de tipo object (cadenas de texto). Aquí hay algunos valores faltantes ya que debería haber 936 entradas.\n",
    "- **Industry:** 936 entradas no nulas de tipo object (cadenas de texto).\n",
    "- **Investors:** 936 entradas no nulas de tipo object (cadenas de texto).\n",
    "- **year:** 936 entradas no nulas de tipo int64 (números enteros).\n",
    "- **month:** 936 entradas no nulas de tipo int64 (números enteros).\n",
    "- **day:** 936 entradas no nulas de tipo int64 (números enteros).\n",
    "\n",
    "Este paso es crucial para entender la calidad de los datos con los que estás trabajando. En particular, puedes ver si hay valores faltantes que debas abordar y si los tipos de datos son los esperados para cada columna. También te da una idea de si es necesario cambiar los tipos de datos de las columnas para su posterior procesamiento o análisis.\n",
    "\n",
    "# Limpieza de los datos\n",
    "\n",
    "En este paso, estás eliminando las columnas 'Company' y 'Date' del DataFrame 'unicornios' utilizando el método `drop()`. Esto es una parte del proceso de limpieza de datos y preparación para el análisis.\n",
    "\n",
    "La columna 'Company' se elimina porque el nombre de la empresa no es relevante para la predicción del valor de la empresa. En el contexto del aprendizaje automático, es un atributo no predictivo. Al eliminar esta columna, simplificas el modelo al no tener que tratar con variables categóricas de alta cardinalidad, que podrían requerir métodos avanzados como la codificación one-hot, la codificación de frecuencias, la incorporación de entidades o el tratamiento de categorías raras.\n",
    "\n",
    "La columna 'Date' se elimina porque ya tienes la información de la fecha desglosada en las columnas 'year', 'month' y 'day'. Es un ejemplo de ingeniería de características, en la que se extraen características más útiles de los datos existentes. Mantener la columna 'Date' en su formato original no proporcionaría ningún beneficio adicional al modelo y solo haría que el conjunto de datos sea más difícil de manejar.\n",
    "\n",
    "Este proceso simplifica el DataFrame y lo prepara para el análisis posterior. Es importante notar que las decisiones sobre qué columnas eliminar dependen del conocimiento específico del dominio, de los datos y de los objetivos de tu análisis o modelado.\n",
    "\n",
    "# Análisis univariante\n",
    "\n",
    "Al llamar al método `describe()` en el DataFrame 'unicornios', estás generando estadísticas descriptivas que resumen la tendencia central, la dispersión y la forma de la distribución del conjunto de datos, excluyendo los valores NaN.\n",
    "\n",
    "Estas estadísticas incluyen lo siguiente para cada columna:\n",
    "\n",
    "- **count:** el número de filas no nulas, que puede ser útil para identificar columnas con valores faltantes.\n",
    "- **mean:** la media de los valores, que es útil para obtener un sentido de la ubicación central de los datos.\n",
    "- **std:** la desviación estándar de los valores, que es útil para entender la cantidad de variabilidad o dispersión en los datos.\n",
    "- **min:** el valor mínimo.\n",
    "- **25%:** el primer cuartil de los datos, donde el 25% de los puntos de datos son más bajos.\n",
    "- **50%:** la mediana de los datos, el valor medio de la distribución.\n",
    "- **75%:** el tercer cuartil de los datos, donde el 75% de los puntos de datos son más bajos.\n",
    "- **max:** el valor máximo.\n",
    "\n",
    "Este es un paso importante del Análisis Exploratorio de Datos (EDA) ya que te permite obtener un entendimiento rápido de la distribución de tus datos.\n",
    "\n",
    "## Histograma de la columna \"Valuation\"\n",
    "\n",
    "Estoy generando un histograma de la columna \"Valuation\", que representa la valoración en miles de millones de dólares de las empresas unicornio. Este gráfico muestra la distribución de los datos, es decir, cómo se distribuyen los valores de \"Valuation\" en todo el conjunto de datos. Al utilizar una escala logarítmica, es más fácil visualizar y entender la distribución de los datos cuando hay una gran variabilidad o diferencias extremas en los valores, como en este caso.\n",
    "\n",
    "## Gráfico de barras de los países con mayor cantidad de startups unicornio\n",
    "\n",
    "Este fragmento de código genera un gráfico de barras que muestra los 10 países con la mayor cantidad de startups unicornio. La función `value_counts()` se utiliza en la columna 'Country' para contar cuántas veces aparece cada país en el conjunto de datos. La función `nlargest(10)` selecciona los 10 países con la mayor cantidad de startups unicornio. Finalmente, `plot(kind='bar')` genera un gráfico de barras a partir de estos datos.\n",
    "\n",
    "## Gráfico de caja de la valoración de las startups unicornio\n",
    "\n",
    "Este fragmento de código genera un gráfico de caja (boxplot) de la valoración de las startups unicornio. Un gráfico de caja es un método estandarizado para representar la distribución de los datos y es especialmente útil para comparar la distribución de los datos entre varios grupos. \n",
    "\n",
    "# Preprocesamiento de Datos (Data Preprocessing)\n",
    "\n",
    "## Verificación de valores nulos\n",
    "\n",
    "Este código verifica si hay algún valor nulo en los datos. En Python, la función `isna().sum()` de pandas se utiliza para contar el número total de valores nulos o faltantes en cada columna de un DataFrame.\n",
    "\n",
    "Los valores nulos o faltantes en los datos pueden ser un problema, ya que pueden afectar los resultados de los análisis y los modelos de Machine Learning. Por lo tanto, es importante identificar y manejar adecuadamente estos valores.\n",
    "\n",
    "En este caso, podemos observar que hay 15 valores nulos en la columna 'City'. El resto de las columnas no tienen valores nulos.\n",
    "\n",
    "Dependiendo de los requerimientos específicos de tu análisis o del modelo de Machine Learning que estés utilizando, podrías querer llenar estos valores nulos con un valor específico, eliminar las filas o las columnas que los contienen, o utilizar alguna otra estrategia para manejar estos valores.\n",
    "\n",
    "## Imputación de valores nulos\n",
    "\n",
    "En esta línea de código, estoy imputando los valores faltantes de la columna 'City' con la moda. La moda es el valor que aparece con mayor frecuencia en un conjunto de datos. En Python, la función `mode()` de pandas se utiliza para calcular la moda de una columna.\n",
    "\n",
    "Estoy utilizando el método `fillna()` para rellenar los valores nulos en la columna 'City' con el valor de la moda. El argumento `inplace=True` se utiliza para realizar los cambios directamente en el DataFrame original.\n",
    "\n",
    "Imputar los valores faltantes con la moda puede ser una buena opción cuando se está trabajando con datos categóricos y se desea mantener la distribución de los datos. Sin embargo, hay que tener en cuenta que esta estrategia puede introducir sesgos si la cantidad de valores nulos es alta en comparación con el tamaño total del conjunto de datos.\n",
    "\n",
    "## Verificación después de la imputación\n",
    "\n",
    "Estoy verificando si aún quedan valores nulos en la columna 'City' después de haber realizado la imputación. Para hacer esto, estoy utilizando la función `isna().sum()`. `isna()` devuelve una Serie de valores booleanos, donde True indica un valor nulo y False un valor no nulo. Al aplicar `sum()` a esta Serie, Python automáticamente convierte los valores True en 1 y los False en 0, y luego suma todos los valores, proporcionando el número total de valores nulos en la columna. En este caso, espero que el número sea 0, indicando que todos los valores nulos se han llenado correctamente.\n",
    "\n",
    "## Limpieza y Estandarización de Datos\n",
    "\n",
    "Estoy reemplazando los valores mal escritos o mal clasificados en la columna 'Industry'. En particular, estoy cambiando todas las entradas que dicen 'Finttech' por 'Fintech'. Esto es importante porque la limpieza y estandarización de los datos son pasos esenciales antes de realizar cualquier análisis más profundo. Al tener los datos limpios y estandarizados, me aseguro de que cualquier análisis o modelo que construya sea lo más preciso y relevante posible.\n",
    "\n",
    "## Corrección de Errores de Escritura\n",
    "\n",
    "He corregido otro error de escritura en la columna 'Industry', cambiando 'Artificial Intelligence' por 'Artificial intelligence'. La consistencia en la nomenclatura de las categorías es vital para mantener la precisión en el análisis.\n",
    "\n",
    "## Análisis de Startups por Industria\n",
    "\n",
    "Luego, he creado una variable llamada 'startups_por_industria' que contiene el conteo de startups por cada industria. Esta información es valiosa para entender en qué industrias se concentran más estas empresas unicornio.\n",
    "\n",
    "Aquí están los resultados:\n",
    "\n",
    "**Número de startups por industria:**\n",
    "\n",
    "- Fintech: 191\n",
    "- Internet software & services: 167\n",
    "- E-commerce & direct-to-consumer: 102\n",
    "- Artificial intelligence: 73\n",
    "- Health: 63\n",
    "- Other: 51\n",
    "- Supply chain, logistics, & delivery: 51\n",
    "- Cybersecurity: 41\n",
    "- Mobile & telecommunications: 37\n",
    "- Data management & analytics: 36\n",
    "- Hardware: 32\n",
    "- Auto & transportation: 29\n",
    "- Edtech: 27\n",
    "- Consumer & retail: 23\n",
    "- Travel: 13\n",
    "\n",
    "Estos datos nos dan una visión general de las industrias en las que las startups unicornio tienden a concentrarse más. Por ejemplo, la industria Fintech parece tener el mayor número de estas empresas, seguida por el software y servicios de internet.\n",
    "\n",
    "## Análisis de Startups por País\n",
    "\n",
    "Primero, he identificado cuántos países únicos hay en la columna 'Country'. Resulta que hay un total de 47 países diferentes donde se encuentran las startups unicornio. Este es un indicador de la diversidad geográfica de estas empresas.\n",
    "\n",
    "Después, he calculado el número de startups por país. De esta lista, se puede ver que las startups unicornio están principalmente concentradas en los Estados Unidos y China, seguidos por India y el Reino Unido. También es importante destacar que hay varias empresas unicornio en países que normalmente no se consideran los principales centros de startups, lo que muestra que la innovación puede surgir de cualquier parte del mundo.\n",
    "\n",
    "## Análisis Multivariante\n",
    "\n",
    "El gráfico que he generado es un mapa de calor de la correlación entre las características numéricas en nuestro conjunto de datos. Este tipo de gráfico es útil para entender rápidamente las relaciones entre nuestras características.\n",
    "\n",
    "En términos simples, una correlación es una medida de la relación entre dos variables. Los valores de correlación van de -1 a 1. Si el valor es cercano a 1, significa que hay una fuerte correlación positiva entre las dos variables. Cuando se acerca a -1, las variables tienen una fuerte correlación negativa.\n",
    "\n",
    "Aquí, estamos utilizando el método .corr() que calcula la correlación de Pearson. Observando el mapa de calor de correlación, se puede ver que no hay una correlación significativa entre las variables numéricas de nuestra data. Esto significa que las características numéricas no están fuertemente relacionadas entre sí.\n",
    "\n",
    "También es importante tener en cuenta que, aunque la correlación puede ayudar a identificar las relaciones existentes entre las variables, una correlación baja no necesariamente significa que las variables son irrelevantes para nuestro modelo. Cada característica todavía puede proporcionar información única que puede ayudar a nuestro modelo a hacer predicciones más precisas.\n",
    "\n",
    "## Análisis Bivariante\n",
    "\n",
    "El gráfico de línea que he generado representa la evolución de la valoración media de las startups unicornio a lo largo del tiempo. El eje X representa los años, mientras que el eje Y muestra la valoración media de las startups unicornio.\n",
    "\n",
    "Observando el gráfico, podemos ver la tendencia de la valoración media de las startups a lo largo de los años. Esto puede dar una idea de cómo ha evolucionado la valoración de las startups unicornio en el pasado, lo que podría ser útil para entender la dinámica del mercado y prever las tendencias futuras.\n",
    "\n",
    "Este tipo de análisis es útil porque nos permite entender mejor cómo se están comportando ciertas características en nuestro conjunto de datos a lo largo del tiempo. Al entender estas tendencias, podemos hacer predicciones más informadas y entender mejor la relación entre las características y la variable objetivo.\n",
    "\n",
    "El gráfico de barras que he creado representa la valoración media de las startups unicornio agrupadas por industria. El eje X muestra las distintas industrias y el eje Y muestra la valoración media de las startups unicornio.\n",
    "\n",
    "Observando el gráfico, podemos ver cuál es la valoración media de las startups unicornio en cada industria. Esto puede ser útil para entender cuáles son las industrias más valiosas en términos de startups unicornio. Además, también nos permite ver la variabilidad entre las diferentes industrias.\n",
    "\n",
    "Este tipo de análisis es útil para entender mejor las diferencias entre los grupos en nuestros datos. Al entender estas diferencias, podemos identificar las características que podrían tener un impacto significativo en la valoración de una startup unicornio.\n",
    "\n",
    "## Ingeniería de Características\n",
    "\n",
    "Lo que acabo de hacer es una parte de la ingeniería de características en la columna 'Investors'. Primero, he obtenido una lista de todos los inversores únicos de la columna 'Investors' utilizando el método unique(). Luego, he recorrido cada fila de la columna 'Investors', he dividido cada cadena de inversores por comas para obtener una lista de inversores para cada startup, y luego he añadido cada inversor individual a la lista 'todos_inversores'. Finalmente, he creado una lista de todos los inversores únicos, 'inversores_unicos', utilizando la función set() para eliminar los duplicados y list() para convertir el conjunto resultante en una lista.\n",
    "\n",
    "Ahora, con esta lista de 'inversores_unicos', tengo una mejor comprensión de los diferentes inversores involucrados en estas startups unicornio y puedo utilizar esta información para futuras tareas de análisis y modelado. Por ejemplo, podría usar esta información para crear nuevas características basadas en la presencia de ciertos inversores, o para analizar el impacto de diferentes inversores en la valoración de las startups.\n",
    "\n",
    "He creado una nueva característica, 'num_investors', que representa el número de inversores de cada startup. Para hacerlo, he aplicado una función lambda a la columna 'Investors' que divide cada cadena de inversores por comas y luego toma la longitud de la lista resultante. Esto da el número de inversores para cada startup.\n",
    "\n",
    "Estoy trabajando con un conjunto de datos de \"unicornios\", que son empresas privadas valoradas en más de mil millones de dólares. Estos datos incluyen características categóricas como 'Country', 'City' y 'Industry'. Sin embargo, muchos algoritmos de aprendizaje automático funcionan mejor con entradas numéricas en lugar de categóricas. Para abordar esto, he decidido transformar estas variables categóricas en un formato que los modelos de aprendizaje automático puedan interpretar más eficientemente.\n",
    "\n",
    "Para hacerlo, he utilizado la función get_dummies de pandas, la cual implementa una técnica llamada codificación One-Hot. Esta función transforma cada valor único en una columna categórica en una nueva columna (o variable ficticia) en el DataFrame. Luego, asigna un valor de 1 o 0 a estas nuevas columnas para cada registro, dependiendo de si el registro tenía ese valor en la columna original.\n",
    "\n",
    "Cuando escribí el código `unicornios = pd.get_dummies(unicornios, columns=['Country', 'City', 'Industry'])`, lo que hice fue transformar las columnas 'Country', 'City' e 'Industry' de mi DataFrame 'unicornios' en múltiples columnas numéricas nuevas. Por ejemplo, si la columna 'Country' originalmente contenía 'USA', 'UK' y 'China', ahora tengo tres nuevas columnas llamadas 'Country_USA', 'Country_UK' y 'Country_China', cada una indicando con un 1 si la empresa es de ese país y un 0 en caso contrario.\n",
    "\n",
    "Al hacer esto, facilito que los algoritmos de aprendizaje automático puedan interpretar y utilizar estas características para aprender de los datos y hacer predicciones precisas.\n",
    "\n",
    "## Escalado de datos\n",
    "\n",
    "Lo que acabo de hacer se llama preprocesamiento de datos, específicamente, la normalización de los datos. He utilizado la función StandardScaler de la biblioteca sklearn, que normaliza las características al restar la media y dividir por la desviación estándar.\n",
    "\n",
    "Este proceso es importante porque algunos algoritmos de machine learning no funcionan bien si las características están en diferentes escalas. Por ejemplo, muchos algoritmos se basan en la distancia euclidiana entre dos puntos de datos, y si una característica está en una escala mucho mayor que otra, la característica con la escala mayor dominará. Al estandarizar las características, podemos evitar este problema y asegurarnos de que todas las características sean tratadas por igual en nuestro algoritmo.\n",
    "\n",
    "En este caso, he normalizado las características 'Valuation' y 'num_investors'. Estas características ahora tendrán una media de 0 y una desviación estándar de 1, lo que nos permitirá utilizarlas en un algoritmo de machine learning que requiere que las características estén en la misma escala.\n",
    "\n",
    "## Identificación de características correlacionadas\n",
    "\n",
    "En este paso, estoy identificando aquellas características que tienen una correlación absoluta con la variable objetivo 'Valuation' mayor que un cierto umbral. En este caso, el umbral es 0.5. Este enfoque nos permite seleccionar aquellas características que tienen una fuerte relación con la variable objetivo, lo que puede mejorar el rendimiento de nuestro modelo de aprendizaje automático.\n",
    "\n",
    "## Establecimiento de un umbral de correlación\n",
    "\n",
    "Después de obtener la correlación de las características con 'Valuation', estoy estableciendo un umbral de 0.2. Este umbral es un criterio arbitrario que he establecido para seleccionar las características que tienen una correlación significativa (tanto positiva como negativa) con 'Valuation'. El valor absoluto se usa aquí para considerar ambas correlaciones positivas y negativas.\n",
    "\n",
    "Después, selecciono los índices de las características que tienen un valor absoluto de correlación mayor que el umbral. Al hacer esto, estoy creando una lista de las características que están más fuertemente correlacionadas con 'Valuation' y que, por lo tanto, son las más relevantes para predecir 'Valuation'.\n",
    "\n",
    "Finalmente, imprimo estas características seleccionadas para revisar cuáles son. Esto me ayuda a comprender mejor cuáles son las características más relevantes en mi conjunto de datos.\n",
    "\n",
    "# Construcción y Evaluación de Modelos\n",
    "\n",
    "## Creación de los Modelos\n",
    "\n",
    "En este paso de mi proyecto, estoy construyendo y evaluando varios modelos de aprendizaje automático para predecir la valoración de los \"unicornios\". He importado una variedad de modelos, incluyendo Regresión Lineal, Regresión Polinomial, Árboles de Decisión, K-Vecinos más Cercanos (KNN), Máquinas de Vectores de Soporte (SVM) y Bosques Aleatorios.\n",
    "\n",
    "Primero, selecciono las características que usaré para entrenar los modelos. Posteriormente, divido mi conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba, utilizando un 80% de los datos para entrenar y el 20% restante para probar.\n",
    "\n",
    "A continuación, creo una lista de los modelos que quiero probar. Luego, entreno cada uno de estos modelos con mis datos.\n",
    "\n",
    "## Evaluación de los Modelos\n",
    "\n",
    "Una vez entrenado cada modelo, los utilizo para predecir las valoraciones de los unicornios en mi conjunto de pruebas. Luego, calculo varias métricas de error para cada modelo, incluyendo el Error Cuadrático Medio (MSE), el Error Absoluto Medio (MAE), la Raíz del Error Cuadrático Medio (RMSE), y el Coeficiente de Determinación, también conocido como R cuadrado. \n",
    "\n",
    "Finalmente, imprimo los resultados de cada modelo. Este paso me ayuda a entender el rendimiento de cada uno y a decidir cuál de ellos es el más prometedor para mi objetivo de predecir las valoraciones de los unicornios. \n",
    "\n",
    "## Validación Cruzada y Pipeline\n",
    "\n",
    "En este paso, decidí realizar validación cruzada en mis modelos para obtener una mejor idea de su rendimiento. He decidido usar 'k=5', lo que significa que estoy dividiendo mis datos en 5 subconjuntos.\n",
    "\n",
    "Además, he incorporado la clase Pipeline de Scikit-Learn para garantizar que la transformación PolynomialFeatures se aplique durante cada iteración de la validación cruzada para la regresión polinomial. \n",
    "\n",
    "A continuación, para cada modelo, realizo la validación cruzada utilizando la función cross_val_score de Scikit-Learn. Finalmente, imprimo los RMSE de cada iteración, así como su media y desviación estándar, para cada modelo.\n",
    "\n",
    "## Optimización de Hiperparámetros\n",
    "\n",
    "Llegados a este punto, decido realizar una optimización de los hiperparámetros de mi modelo de bosques aleatorios. Para hacer esto, empleo la técnica de búsqueda de cuadrícula, que prueba todas las posibles combinaciones de los hiperparámetros que quiero ajustar. \n",
    "\n",
    "# Optimización de Hiperparámetros y Predicción con el Modelo Optimizado\n",
    "\n",
    "## Búsqueda de Cuadrícula para Optimizar Hiperparámetros\n",
    "\n",
    "Para realizar la búsqueda de cuadrícula, creo un diccionario 'param_grid' con los hiperparámetros y los valores que quiero probar. A continuación, ajusto la búsqueda de cuadrícula a mis datos de entrenamiento.\n",
    "\n",
    "Finalmente, imprimo los mejores hiperparámetros encontrados y extraigo el mejor modelo resultante de la búsqueda de cuadrícula. \n",
    "\n",
    "## Predicción con el Modelo de Bosque Aleatorio Optimizado\n",
    "\n",
    "Aplico el modelo de Bosque Aleatorio (Random Forest) optimizado para hacer predicciones sobre mi conjunto de prueba. \n",
    "\n",
    "Primero, entreno este modelo optimizado con mi conjunto de datos de entrenamiento. Una vez que el modelo está entrenado, lo utilizo para hacer predicciones en el conjunto de datos de prueba.\n",
    "\n",
    "A continuación, calculo varias métricas de rendimiento para evaluar la eficacia de mi modelo y finalmente, imprimo estas métricas.\n",
    "\n",
    "# Guardar el Modelo Entrenado\n",
    "\n",
    "En esta última etapa de mi trabajo, decido guardar el modelo que entrené para su uso futuro. Para guardar mi modelo, utilizo la biblioteca joblib y su función dump. \n",
    "\n",
    "Después de ejecutar este código, tengo un archivo llamado 'new_model.joblib' que contiene todo lo que necesito para cargar mi modelo y hacer predicciones en el futuro.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
